import os
import pathlib

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import models
from IPython import display 


# Set the seed value for experiment reproducibility.
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)


DATASET_PATH = 'data/mini_speech_commands'

data_dir = pathlib.Path(DATASET_PATH)
if not data_dir.exists():
  tf.keras.utils.get_file(
      'mini_speech_commands.zip',
      origin="http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip",
      extract=True,
      cache_dir='.', cache_subdir='data')

commands = np.array(tf.io.gfile.listdir(str(data_dir)))
commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]
#print('Commands:', commands)

train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(
    directory=data_dir,
    batch_size=64,
    validation_split=0.999,
    seed=0,
    output_sequence_length=16000,
    subset='both')

label_names = np.array(train_ds.class_names)
#print()
#print("label names:", label_names)

train_ds.element_spec

def squeeze(audio, labels):
  audio = tf.squeeze(audio, axis=-1)
  return audio, labels

train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)
val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)

test_ds = val_ds.shard(num_shards=2, index=0)
val_ds = val_ds.shard(num_shards=2, index=1)



for example_audio, example_labels in train_ds.take(1):  
 print(example_audio.shape)
 print(example_labels.shape)


label_names[[1,1,3,0]]

plt.figure(figsize=(16, 10))
rows = 3
cols = 3
n = rows * cols


#=====Plots the waveform========
#for i in range(n):
#  plt.subplot(rows, cols, i+1)
#  print("alive and didnt crash")
#  audio_signal = example_audio[i]
#  plt.plot(audio_signal)
#  plt.title(label_names[example_labels[i]])
#  plt.yticks(np.arange(-1.2, 1.2, 0.2))
#  plt.ylim([-1.1, 1.1])


###########Spectrogram-step###########
def get_spectrogram(waveform):
 # Convert the waveform to a spectrogram via a STFT.
 spectrogram = tf.signal.stft(
 waveform, frame_length=255, frame_step=128)
 # Obtain the magnitude of the STFT.
 spectrogram = tf.abs(spectrogram)
  # Add a `channels` dimension, so that the spectrogram can be used
  # as image-like input data with convolution layers (which expect
  # shape (`batch_size`, `height`, `width`, `channels`).
 spectrogram = spectrogram[..., tf.newaxis]
 return spectrogram





#Appends the values to the label, waveform and spectrogram, an example
for i in range(3):
 label = label_names[example_labels[i]]
 waveform = example_audio[i]                     #Takes waveform and converts it to spectrogram via the get_spectrogram function 
 spectrogram = get_spectrogram(waveform)




#print('Label:', label)
#print('Waveform shape:', waveform.shape)
#print('Spectrogram shape:', spectrogram.shape)
#print('Audio playback')
#display.display(display.Audio(waveform, rate=16000))

def plot_spectrogram(spectrogram, ax):
 if len(spectrogram.shape) > 2:
  assert len(spectrogram.shape) == 3
  spectrogram = np.squeeze(spectrogram, axis=-1)
  #Convert the frequencies to log scale and transpose, so that the time is
  #represented on the x-axis (columns).
  #Add an epsilon to avoid taking a log of zero.
 log_spec = np.log(spectrogram.T + np.finfo(float).eps)
 height = log_spec.shape[0]
 width = log_spec.shape[1]
 X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)
 Y = range(height)
 ax.pcolormesh(X, Y, log_spec)

  
#fig, axes = plt.subplots(2, figsize=(12, 8))
#timescale = np.arange(waveform.shape[0])
#axes[0].plot(timescale, waveform.numpy())
#axes[0].set_title('Waveform')
#axes[0].set_xlim([0, 16000])

#plot_spectrogram(spectrogram.numpy(), axes[1])
#axes[1].set_title('Spectrogram')
#plt.suptitle(label.title())
#plt.show()


#function to transfer the audio data from waveform to spectrogram and aply the Fourier transform in datasets.
def make_spec_ds(ds):
  return ds.map(
    map_func=lambda audio,label: (get_spectrogram(audio), label), num_parallel_calls=tf.data.AUTOTUNE)



#transfer dataset data from waveform to spectrogram, in this case in train_dataset
train_spectrogram_ds = make_spec_ds(train_ds)


set_sample = 3  #the output track should be the number 
#Function to get audio values before the Fourier transform:
for audio, label in train_ds.take(1):
  print("---------------------------------------------")
  single_audio_sample_preprocessed = audio.numpy()[set_sample]
  print("Audio values before transfer:", single_audio_sample_preprocessed)
  clttas0 = label.numpy()[set_sample]
  print("Corresponding label:", clttas0)
  print("=============================================")

#Function to get audio values after the Fourier transform:
for audio, label in train_spectrogram_ds.take(1):
  print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
  single_audio_sample_processed = audio.numpy()[set_sample]
  print("Audio values after the transfer:", single_audio_sample_processed)
  clttas1  = label.numpy()[set_sample]
  print("Corresponding label", clttas1)
  print("++++++++++++++++++++++++++++++++++++++++++++++++")


#output data into files 
output_file0 = "preprocessed_track.txt"
with open(output_file0, "w") as file:
    for value in single_audio_sample_preprocessed:
        file.write(f"{value}\n")

output_file1 = "processed_track.txt"
with open(output_file1, "w") as file:
    for value in single_audio_sample_processed:
        file.write(f"{value}\n")
